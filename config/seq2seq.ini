[strings]
# Mode : train, test, serve
mode = train
seq_data = C:/Users/JGanson/Desktop/Deep Learning/largebot/train_data/seq.data
seq_test_data = C:/Users/JGanson/Desktop/Deep Learning/largebot/train_data/seq_test.data
train_data=train_data

running_data = C:/Users/JGanson/Desktop/Deep Learning/largebot/train_data/data_running.pt
resource_data = C:/Users/JGanson/Desktop/Deep Learning/largebot/train_data/dialogues_train.txt
dict = C:/Users/JGanson/Desktop/Deep Learning/largebot/train_data/wordlist.txt

e = E
m = M

model_data = C:/Users/JGanson/Desktop/Deep Learning/largebot/train_data/data_model.pt
[ints]
# vocabulary size 
# 	20,000 is a reasonable size
enc_vocab_size = 20000
dec_vocab_size = 20000
embedding_dim=128
max_length=20
# typical options : 128, 256, 512, 1024
layer_size = 256
# dataset size limit; typically none : no limit
max_train_data_size = 100000
batch_size = 30
[floats]

min_loss=0.03

